---
layout: post
title: "Hyperparameter Optimization Libraries"
categories: [Data Science]
tags: [Python]
image:
  path: /assets/images/hyper.png
---
## Hyperparameter Optimization Libraries


### Ray Tune
- **Usage Areas**: Ray Tune is designed for hyperparameter optimization of machine learning models, particularly effective in large datasets and distributed systems. Integrated with Ray's general-purpose parallelism library, it enables highly parallel and scalable tuning processes.
- **Example**:
  ```python
  import ray
  from ray import tune

  def train_model(config):
      # Model training logic
      for i in range(10):
          # Simulate a training loop
          loss = (config["alpha"] - i) ** 2
          tune.report(loss=loss)  # Send the current loss back to Ray Tune

  analysis = tune.run(
      train_model,
      config={
          "alpha": tune.grid_search([0.001, 0.01, 0.1, 1, 10])
      }
  )

  print("Best config: ", analysis.get_best_config(metric="loss"))
```
### 1. Scikit-learn (`GridSearchCV`, `RandomizedSearchCV`)
- **Usage Areas**: Widely used for its `GridSearchCV` and `RandomizedSearchCV` modules, Scikit-learn performs exhaustive searches through a grid of parameters with `GridSearchCV`, and random searches across parameter combinations using `RandomizedSearchCV`. Ideal for small to medium-sized datasets.
- **Example**:
  ```python
  from sklearn.model_selection import GridSearchCV
  from sklearn.svm import SVC
  param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
  grid_search = GridSearchCV(SVC(), param_grid, cv=3)
  grid_search.fit(X_train, y_train)
  print("Best parameters:", grid_search.best_params_)
```
### 2. Hyperopt
Usage Areas: Utilizing Bayesian optimization, Hyperopt explores the best regions in the parameter space using a probabilistic model. Suitable for large parameter spaces and can perform more efficient searches.
Example:
```python
from hyperopt import hp, fmin, tpe, Trials
space = {'C': hp.uniform('C', 0, 10), 'kernel': hp.choice('kernel', ['linear', 'rbf'])}
best = fmin(fn=objective_function, space=space, algo=tpe.suggest, max_evals=100)
print("Best parameters:", best)
```
### 3. Optuna
Usage Areas: Designed for broader and more complex hyperparameter spaces, Optuna allows for dynamic adjustment of parameters with each trial. Particularly useful for numerous trials and deep learning models.
Example:
  ```python
import optuna
def objective(trial):
    C = trial.suggest_float('C', 0.1, 10)
    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])
    svc = SVC(C=C, kernel=kernel)
    return cross_val_score(svc, X_train, y_train, n_jobs=-1, cv=3).mean()
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
print("Best parameters:", study.best_trial.params)
```
### 4. Spearmint
Usage Areas: Implements Bayesian optimization to tune hyperparameters. Like Hyperopt, it aims for optimal solutions with fewer trials, especially for costlier evaluations.
Example:

# Example usage typically involves setting up a configuration file and running the Spearmint process externally.
### 5. Bayesian Optimization
Usage Areas: This library manages complex hyperparameter tuning processes using Bayesian optimization techniques, particularly effective in experimental designs where parameter tuning is crucial.
Example:
```python
from bayes_opt import BayesianOptimization
def objective_function(C, gamma):
    return cross_val_score(SVC(C=C, gamma=gamma), X_train, y_train, cv=3).mean()
optimizer = BayesianOptimization(f=objective_function, pbounds={'C': (0.1, 10), 'gamma': (0.001, 1)}, random_state=1)
optimizer.maximize(init_points=2, n_iter=10)
```